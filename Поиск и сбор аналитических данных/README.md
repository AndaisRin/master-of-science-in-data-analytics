В репозитории собраны проекты, выполненые в рамках обучения по предмету "Поиск и сбор аналитических данных".

| Скрипт| Назначение | Ключевые параметры / ввод | Вывод / результат| Примечания|
| :----- | :---------- | :---------- | :---------- | :---------- |
| `01_cats_exhibition.py` | Сбор выставок кошек с сайта **ru-pets.ru**: дата, название, организатор. | Загружает все страницы каталога, парсит блоки `.listitem`. Использует `requests`, `BeautifulSoup`, `re`. Пауза `time.sleep(1)` между запросами. | CSV `exhibitions.csv`: `Дата проведения`, `Название выставки`, `Клуб-Организатор`. | Логирование через `logging`. Корректирует кодировку `windows-1251`. |
| `02_laptop_wb.py` | Извлечение характеристик ноутбука с Wildberries через Selenium. | URL карточки товара (по умолчанию ноутбук с `id=216378094`). Парсит таблицу характеристик `.product-params__table`. | JSON в stdout с полями (ОС, процессор, RAM, экран, вес и т.д.) только из списка `required_specs`. | Зависимости: `selenium`, `webdriver_manager`. Запускает Chrome, без headless по умолчанию. |
| `03_cafe_tomsk.py` | Сбор всех ресторанов Томска с сайта **zoon.ru** через подгрузку «Показать ещё». | Selenium + headless Chrome. Жмёт кнопку «Показать ещё» до конца списка. Извлекает: название, рейтинг, направления.  | CSV `tomsk_restaurants.csv`: `Название`, `Рейтинг`, `Направления`. | Логирование через `logging`. Использует ожидания (`WebDriverWait`) и обработку исключений. |
| `03_cafe_tomsk_light.py` | Упрощённая версия предыдущего парсера без подгрузки, только видимые карточки. | Загружает страницу `zoon.ru/tomsk/restaurants/`, парсит карточки `li.minicard-item.js-results-item`. | CSV `tomsk_restaurants.csv` с теми же полями. | Нет обработки пагинации, работает быстрее, но данные неполные. |
| `04_cheese.py` | Сбор ассортимента мезофильных заквасок с сайта **pro-syr.ru**. | Загружает страницу каталога, кликает «Показать ещё» пока кнопка доступна. Использует Selenium + BeautifulSoup. | CSV `zakvaski_prosyr_full.csv`: `Название продукта`, `Цена`, `Наличие`. Параллельно печатает список в stdout. | Headless Chrome, обработка кликов через JS. |
| `05_data_quality.ipynb` | Ноутбук для проверки качества собранных данных. | Зависит от конкретного наполнения (анализ CSV). | Графики, метрики, выводы. | | Требует `pandas`, `matplotlib` и пр. |
| `06_final_tpoger.py` | Парсинг статей с **tproger.ru** за последние 2 месяца: заголовок, дата, описание, лайки, комментарии. | Автоматическая прокрутка страницы Selenium до нужных дат. BeautifulSoup парсит блоки `.tp-ui-post-card`. | CSV `tproger_articles_YYYYMMDD.csv` с полями: `url`, `date`, `title`, `description`, `likes`, `comments`. | Логирование (`DEBUG/INFO`). Учитывает часовой пояс UTC+3. |
| `hh_ru_api.py` | Получение списка работодателей по региону (Алтайский край) с активными вакансиями и выгрузка ссылок на вакансии. | `area_id` (по умолчанию `1217`), `max_employers`, `per_page` в функциях `get_employers` и `get_vacancy_links`. Пагинация по API HH + небольшая задержка `time.sleep(0.1)`. | CSV `employers_altai_krai.csv` с колонками: `id`, `name`, `vacancy_links`, `open_vacancies` (отсортировано по убыванию вакансий). | Зависимости: `requests`, `csv`. Работает без токена (публичные эндпоинты HH). Учитывает пагинацию `/employers` и `/vacancies`. |
| `kinopoisk_api.py` | Сбор до 1000 фильмов по критериям: год = 2000, жанр = «комедия», сортировка по `rating.kp` (убывание), сохранение базовых полей. | Требуется `X-API-KEY`. Параметры запроса: `year`, `genres.name="+комедия"`, `sortField=rating.kp`, `sortType=-1`, `limit=100` и пагинация `page`. Есть `time.sleep(0.5)` между запросами. | JSON `kinopoisk_comedy_2000.json` (до 1000 записей) с полями: `название`, `длительность` (`movieLength`), `страна производитель` (склеенные страны).  | Зависимости: `requests`, `json`. Проверьте лимиты/синтаксис API в актуальной документации Кинопоиска. |
| `rick_and_morty_api.py` | Утилиты для Rick and Morty API: подсчёт Male/Female на 1-й странице и сбор всех имён по статусу (`alive/dead/unknown`) по всем страницам.  | Функции: `get_male_female_count()` делает GET `.../character?page=1`; `get_character_by_status(status)` итерирует страницы по параметру `status` до отсутствия `info.next`. | Печать в stdout: словарь с количеством полов и список имён + итоговое количество. | Зависимость: `requests`. Без ключей (публичный API). Обработка окончания страниц по коду ответа/`info.next`. |
| `superjob_api.py` | Поиск вакансий «Аналитик» за последнюю неделю на SuperJob и сохранение расширенной информации. | Требуется `API_KEY`. Параметры: `keyword`, `period`, `page`, `count` с обработкой поля `more` для пагинации.  | CSV `vacancies.csv`: ссылка, название, работодатель, город, зарплата (строкой и границы), обязанности, дата публикации (конвертируется из unix time), признак архива. | Зависимости: `requests`, `csv`, `datetime`. Поле зарплаты формируется из `payment_from`/`payment_to`. |
| `vk_research.py` | Поиск сообществ ВК по ключевым словам в указанном городе (Омск), сортировка по подписчикам, извлечение телефонов из контактов, сохранение в CSV. | Требуется `TOKEN` и версия API `5.199`. Получение `city_id` через `database.getCities`; поиск через `groups.search` с `extended=1` и полями `description,is_closed,members_count,contacts`. Извлечение телефонов из `contacts` (включая поиск по regex в `desc`). | CSV `vk_groups.csv`: `id`, `name`, `description`, `is_closed`, `members_count`, `contacts` (номера через `; `). В stdout — счётчики и время выполнения. | Зависимости: `requests`, `csv`, `re`, `time`, `sys`. Ключевые слова по умолчанию: «цветы», «флористика», «магазин цветов». Город — «Омск». |
| `weather_api.py` | Получение текущей погоды по городу (пример: Смоленск) из WeatherAPI и вывод температуры/«ощущается как». | Требуется `API_KEY`. Параметры: `key`, `q=CITY`, `aqi=no`. Эндпоинт: `/v1/current.json`. Город задаётся строкой. | Печать в stdout: `Город`, `Температура (°C)`, `Ощущается как (°C)`. | Зависимость: `requests`. Убедитесь, что ключ активен и тарификация позволяет запросы. |
